Lab 7
by David Krug and Nathanial Imel

Lab7/ttv_initial = Lab6/ttv_final
   But Lab7/ttv_initial/tsdb/home still has the four tsdb profiles from lab 6.

To do:
   non-verbal predicates
      make testsuite examples
      	   for NP NP
      implement them using TDL
         this requires implementing PPs (if titan has them)
      test our implementation by parsing the testsuite examples
   machine translation/other testing
      test translation of lines 15-17
      test wh questions in non-verbal predicate sentences   
   grammar comparisions
      run the final grammar with the testsuite/test corpus
   other small fixes
      double check that semantical-empty entries and spurious pronouns are ALL gone
      _pron_n_rel -> pron_rel
   finish the writeup
      what if we put the writeup through a spell checker

Questions for Emily:
	  For the pron/clitic ambiguity, we might want to make it so that
	  sentences that have a main verb MUST have an agreement clitic.
	  How would we implement this in the grammar?


Table of contents (idk if this structure makes sense; it might change):
   1 Noun predicates
      1.1 How they work in Titan
      1.2 How we implemented them
   2 Adjective predicates
      2.1 How they work in Titan
      2.2 How we implemented them
   3 PP predicates
      3.1 How they work in Titan
      3.2 How we implemented them
   4 Non-verbal predicate testing
      4.1 Parsing non-verbal predicates.
      4.2 translation for lines 15-17 in ttv.txt and translation testing
      4.3 testing wh questions in non-verbal predicate sentences
   5 Small changes
      any thing else we change in the grammar that is worth writing up
   6 Grammar comparision
      6.1 Testsuite
         6.1.1 Initial grammar
         6.1.2 Final grammar
      6.2 Test corpus
         6.2.1 Initial grammar
         6.2.2 Final grammar

===============================================================================
1 Noun predicates
1.1 How they work
   There is no copula in Titan. 
   oi cinal
   2sg devil
   You are a devil

   ?oi a cinal
   2sg 2sg.real devil
   You are a devil.

1.2 How we implemented them
   We tried using the n-bar rule, bit it didn't work.

===============================================================================
2 Adjective predicates
2.1 How they work
   Adjectives mostly act like intransitive verbs and appear in verbal slots:
      Lodianum ei i likom.
      house-inside 2sg.ps 2sg be.pitch-dark
      The inside of her house was pitch black.
    
   The difference between regular intransitive verbs and adjectives is that
      agreement clitics do not appear with non-third person subjects and
      predicative adjectives:
      
      *oi a        muan
      2sg 2sg.real be.bad
      You are bad
      
      Oi  muan
      3sg be.bad
      You are bad
    
   Predicative adjectives do not appear in irrealis or perfective clauses:
      *manuai ki      muan
      osprey  3sg.irr bad
      The osprey is bad

      *manuai kine     muan
      osprey  3sg.perf bad.
      The osprey is bad.

2.2 How we implemented them
   We didn't change anything.
      Later i might clean up adjectives by fixing the pred values, but it is
      low priority right now.

===============================================================================
3 PP predicates
3.1 How they work
   Titan has only one adposition, the preposition 'e' which is glossed variably
      as of, in, on, etc:

      nat   e  wei  poen
      child in that not
      The child is not in that [basket].
      
      ?e   wei  nat.
      prep that child.
      In that [thing] is a child.

3.2 How we implemented them
   We're going to need two lexical entries for 'e':
      The first is for possessives.
      The second is for non-possessive use of the prep.
      What about pressessives predicates? Predicative possession?

      for "manuai e manuai"
      	  cat     on cat
	  "the cat is on the cat"

      We edited the norm-adpos-lex rule to select for a subject and ID its
      INDEX with the INDEX of the adoposition's ARG1.
      
      	    <insert norm-adpos-lex>

      This allowed us to
      parse the string -- however there was still much ambiguity (7 trees).
      One of those trees was the right analysis, the other 6 all used the
      n-bar rule and the HEAD-ADJ or the ADJ-HEAD rule. We have no adjectives.

      So we deleted three rules from rules.tdl.

      	 < insert the three rules we deleted from rules.tdl>

===============================================================================
4 Non-verbal predicate testing
4.1 Parsing non-verbal predicates

4.2 Machine translation testing
   Here are the translations for lines 15-17:
   
   Cong   i        ani muiny.
   hunger 3sg.real eat dog
   Hunger eats the dogs. (The dogs are hungry.)
   
   Manuai e    poany.
   osprey prep earth
   The osprey is on the earth (The cat is in the park.)
   
   Muiny ala manuai.
   dog   3pl.real osprey
   The dogs are the osprey. (The dogs are the cats.)
   
4.3 Non-verbal predicates in wh questions

===============================================================================
5 Small miscellaneous changes

We continued cleaing up the spurious pronouns cleanup by deleting noun41 to
   noun82 EXCEPT noun48 and noun82. Here's an example of one of the pronouns
   we deleted:

   noun41_name=noun41
   noun41_pron=on
     noun41_feat1_name=number
     noun41_feat1_value=du
   noun41_det=opt
     noun41_stem1_orth=damolou
     noun41_stem1_pred=_pron_n_rel

We also deleted the alternate spelling on 'pa' as 'poa' and instead added a new
   lexical entry for 'poa' that had the appropriate 2sg agreement (w/ subject)
   features.

In our testsuite.txt, we changed mood glosses from nfut/fut to real/irr.
   This is to reflect our analysis that Titan does not have tense, only mood.

We finished spellling out properites in semi.vpm such as cond -> conditional.

We changed the PRED values for some lexical entries used in machine
   translation. 'manuai' changed from _osprey-or-osprey-feather_n_rel to
   _cat_n_rel, voliliti changed from follow to chase, dilen from canoe to
   car, and poany from earth-or-ground to park.


Deleted the auto-inferred:

ca_1 := noun1-noun-lex &
  [ STEM < "ca" >,
    SYNSEM.LKEYS.KEYREL.PRED "_what_n_rel" ].

Changed roots.tdl to have HEAD +vp instead of just HEAD verb

Changed rules.tdl to have three less rules, as described in the PP Predicates
section:

   adj-head-scop := adj-head-scop-phrase.

   head-adj := my-head-adj-phrase.

   adj-head := my-adj-head-phrase.


We got wh-questions to work, and we got them to work as a nominal predicate.
To do this, we edited the SPEC requirement on the wh-pronoun-lex to be nonempty,
instead of empty. Previously, the immediate node above a wh-pronoun like "ca"
was an NP -- now its an N and the BARE-NP rule applies to it to become an NP

So our rule looks like:

wh-pronoun-noun-lex := basic-wh-word-lex & norm-hook-lex-item & basic-icons-lex-item & non-mod-lex-item & zero-arg-que &
  [ SYNSEM [ LOCAL [ CAT [ HEAD noun,
                           VAL [ SPR cons,
                                 SUBJ < >,
                                 COMPS < >,
                                 SPEC < > ] ],
                     CONT [ RELS.LIST < [ LBL #larg,
                                          ARG0 #arg0 & ref-ind ],
                                        quant-relation &
                                        [ PRED "which_q_rel",
                                          ARG0 #arg0,
                                          RSTR #harg ] >,
                            HCONS.LIST < [ HARG #harg,
                                           LARG #larg ] > ] ],
             NON-LOCAL.QUE.LIST < #arg0 > ] ].

===============================================================================
6 Grammar comparision
6.1 Testsuite
   6.1.1 Initial grammar (copied from lab 6)
      56/88 positive items parsed (63.6% coverage)
      6/77 negative items parsed (7.8 overgeneration)
      
      On average:
         positive items that parsed had 1.11 parses.
         negative itmes that parsed had 1.00 parses
      
      The most ambiguous item had 3 parses:
         Ala      kip.
         3pl.real lie
         They lied.
      See lab 6 for an explination of the ambiguity.

   6.1.2 Final grammar
      #/# items parsed.
      #/# positive items parsed (#% coverage)
      #/# negative items parsed (#% overgeneration)
      
      On average:
         Positive items that parsed had # parses.
         Negative items that parsed had # parses.
      
      The most ambiguous items had # parses:
         <IGT>
      <explanation of the ambiguity>

6.2 Test corpus
   6.2.1 Initial grammar (copied from lab 6)
      29/562 items parsed (5.2% coverage)
      
      On average, parsed items had 1.45 prases
      
      The most ambiguous item had 3 parses:
         ala      va  yota     ka  mulie
         3pl.real say 1pl.incl irr run
         They said lets turn around
      See lab 6 for an explanation of the ambiguity.

   6.2.2 Final grammar
      #/# items parsed (#% coverage)
      
      On average, parsed items had # parses.
      
      The most ambiguous item had # parses:
         <IGT>
      <explanation of the ambiguity>
