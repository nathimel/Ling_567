Lab 7
by David Krug and Nathanial Imel

To do:
   grammar comparisions: run the final grammar with the testsuite/test corpus
   finish the writeup
      what if we put the writeup through a spell checker

Table of contents:
   1 Nominal predicates
      1.1 How they work in Titan
      1.2 How we implemented them
   2 Adjectival predicates
      2.1 How they work in Titan
      2.2 How we implemented them
   3 Prepositional predicates
      3.1 How they work in Titan
      3.2 How we implemented them
   4 Non-verbal predicate testing
      4.1 Parsing non-verbal predicates.
      4.2 Translation testing
      4.3 Wh questions and non-verbal predicates
   5 Small miscellaneous changes
   6 Grammar comparision
      6.1 Testsuite
         6.1.1 Initial grammar
         6.1.2 Final grammar
      6.2 Test corpus
         6.2.1 Initial grammar
         6.2.2 Final grammar

===============================================================================
1 Nominal predicates
1.1 How they work in Titan
   There is no copula in Titan, and agreement clitics do not occur with nominal
      predicates:
      
      manuai cinal
      osprey devil
      The osprey is a devil.
      
      oi  cinal
      2sg devil
      You are a devil

      *oi a        cinal
      2sg 2sg.real devil
      You are a devil.

1.2 How we implemented them
   At first we copied the n-bar-predicate-rule from Halkomelem in 2013. This
      resulted in the rule over-firing, and after interactive debugging on
      Thursday we added the MOD <> requirement on the HEAD as well as the
      SPEC <> and no new quantifier requirement. This is the final tdl:

   np-predicate-rule := unary-phrase & nocoord &
     [ SYNSEM [ LOCAL.CAT [ HEAD verb & [MOD < >],
                      VAL [ COMPS < >,
      		 SPEC < >,
      		 SUBJ < [ LOCAL [ CONT.HOOK.INDEX #arg1,
      				  CAT [ HEAD noun,
      					VAL.SPR < > ] ] ] > ] ],
                NON-LOCAL #nl ],
       C-CONT [ HOOK [ LTOP #ltop,
      	    INDEX #index,
      	    XARG #arg1 ],
           RELS.LIST < arg12-ev-relation &
      		 [ PRED "_be_v_id_rel",
      		   LBL #ltop,
      		   ARG0 #index,
      		   ARG1 #arg1,
      		   ARG2 #arg2 ] >,
           HCONS.LIST < > ],
       ARGS < [ SYNSEM [ LOCAL [ CAT [ HEAD noun,
      			    VAL.SPR < > ],
      		      CONT.HOOK [ INDEX #arg2 ]],
      	      NON-LOCAL #nl ] ] > ].

   We added the corresponding rule instatiation in rules.tdl:
      np-pred := np-predicate-rule.
   
   An interesting effect of this is that an NP will now parse as a complete
      sentence. Because the np-predicate-rule turns NPs into VPs, and because
      subjects are optional, a sentence like "manuai" will parse:
      
      Manuai
      osprey
      Something is being an osprey.

   In that sentences:
   arg1 of be_v_id_rel is nonfocus.
   arg2 of be_v_id_rel is a cat that exists.
   The event on the index of the sentence is a prop-or-question.

===============================================================================
2 Adjectival predicates
For this lab, we did not fully implement predicative adjectives. The auto-
   inference system placed all adjectives in the transitive verb class, but
   they should be in the intransitive verb class. Additionally, we have still
   not decided if we should allow non-third verbal clitics. We currently have
   one toy example of a verb, 'dili' (sad).

2.1 How they work in Titan
   Adjectives can appear attributively or predicatively:
      Yo  u        ani kan  muan
      1sg 1sg.real eat food bad
      I eat bad food
      
      Kan  muan.
      food bad
      The food is bad.

   Predicative adjectives mostly act like intransitive verbs and appear in
      verbal slots:
      
      Lodianum ei i likom.
      house-inside 2sg.ps 2sg be.pitch-dark
      The inside of her house was pitch black.
   
   According to the descriptive grammar, the difference between regular
      intransitive verbs and adjectives is that agreement clitics do not appear
      with predicative adjectives if the subject is non-third:
      
      *oi a        muan
      2sg 2sg.real be.bad
      You are bad
      
      Oi  muan
      3sg be.bad
      You are bad
   
   However, the corpus includes the following:
      Yo  u        qadi.
      1sg 1sg.real sick
      I am sick.
    
   Additionally, the descriptive grammar says that predicative adjectives do
      not appear in irrealis or perfective clauses. However, the corpus
      includes:
      
      kor kine alau
      land 3sg.perf far
      "The land is far away.

2.2 How we implemented them
   For this lab, we are analyzing adjectives as a simple intransitive.
      Therefore, they will still accept the agreement clitic in
      non-third person. We might come back to this in the future and ban
      agreement clitics with non-third subjects by using position classes
      like your reccommended in office hours on Friday.
   
   We also noticed that all adjective-like verbs, such as 'dili' (sad)
      inherint from verb1-verb-lex, which are transitive verbs. We also noticed
      that all these verbs have a stem that ends in "~", such as:
           
      dili_7E := verb1-verb-lex &
        [ STEM < "dili~" >,
          SYNSEM.LKEYS.KEYREL.PRED "_be_sad_v_rel" ].

   All adjectives should have the "~" removed from their stem and be moved into
      verb3-verb-lex, which is intransitive. We have not yet fixed all these
      adjectives, but we did fix 'dili' (small):

      dili := verb3-verb-lex &
        [ STEM < "dili" >,
          SYNSEM.LKEYS.KEYREL.PRED "_be_sad_v_rel" ].

===============================================================================
3 Prepositional predicates
3.1 How they work in Titan
   Titan has only one adposition, the preposition 'e' which is glossed variably
      as of, in, on, etc:

      nat   e  wei  poen
      child in that not
      The child is not in that [basket].
      
      muiny e  manuai.
      dog   in osprey
      The dog is in the osprey.
   
   Agreement clitics do not appear with prepositional predicates:
      *Manuai i   e    muiny.
      osprey  3sg prep dog
      The osprey is a dog
    
   We noticed this PPs showing up at the front of sentences, but we think that
      is not predicative prepositions but instead a different contstruction for
      topicalization:
      
      e         si  pein        i   moele   i
      regarding one young-women 3sg adorned 3sg
      As for one young worman, she adorned herself.

   We are curious how this will interact with NP predicates, such as:
      e muiny manuai
      prep dog osprey.
      Regarding the dog, it is an osprey.
   Current all parses of this sentence mean "On the dog, something is being an
      osprey".

3.2 How we implemented them
   Initially, Titan had no prepositions. We added
      one preposition:

   e := normadp1-norm-adposition-lex &
     [ STEM < "e" >,
       SYNSEM.LKEYS.KEYREL.PRED "_loc_p_rel" ].

   With this change, PPs such as the following were being partially parsed as
      NPs:      

      muiny e manuai
      dog in osprey
      the dog in the osprey
   
   To get predicative prepositions working, we edited the norm-adposition-lex
      rule to select for a subject and ID its INDEX with the INDEX of the
      adoposition's ARG1. Here is the final tdl for norm-adposition-lex:

norm-adposition-lex := norm-sem-lex-item & no-hcons-lex-item & basic-intersective-mod-lex & non-local-none-lex-item &
  [ ARG-ST < #comp >,
    SYNSEM [ LKEYS.KEYREL arg12-ev-relation &
                          [ ARG1 #arg1,
   		    ARG2 #arg2 ],
             L-QUE #lque,
             LOCAL [ CONT [ HOOK [ XARG #arg1 ] ],
                     CAT [ WH.BOOL -,
                           VAL [ SPR < >,
                                 SPEC < >,
                                 SUBJ < [ LOCAL [ CAT cat-sat &
                                  	  	    	    [ VAL [ SPR < >,
                                    		    COMPS < > ],
                             			     HEAD noun ],
   				          CONT.HOOK.INDEX #arg1 ] ] >,
                                 COMPS < #comp &
                                         [ L-QUE #lque,
   				   OPT -,
                                           LOCAL [ CAT [ HEAD noun,
                                                         VAL.SPR < > ],
                                                   CONT.HOOK.INDEX #arg2 ] ] > ],
                           HEAD adp &
                                [ MOD < [ LOCAL.CAT [ VAL.SPR cons,
                                                      WH.BOOL -,
                                                      HEAD.AUX - ] ] > ] ] ] ] ].

normadp1-norm-adposition-lex := norm-adposition-lex &
  [ SYNSEM.LOCAL.CAT.HEAD.INIT + ].

   We also changed roots.tdl to have HEAD +vp instead of just HEAD verb.
      
   With these changes, "muiny e manuai" had 7 parses. One was the desired
      parse:
      
      Muiny e  manuai.
      dog   in osprey.
      The dog is in the osprey.
   
   Of the other 6 parses, some had the over-firing np-predicate problem that
      we fixed in section 1.2 rule, and some parses had the HEAD-ADJ or the
      ADJ-HEAD rule applying. To fix this, we added OPT - to the first item
      on the COMPS list of the norm-adpos-lex.

   This reduced the number of parses of "muiny e manuai" to 3 desired parses
      with 3 structures:
      
      Something is being "the dog on the cat".
      Something is being the dog, and that being is happening on the cat.
      A dog is on the cat. (The top node in this tree is a PP)

===============================================================================
4 Non-verbal predicate testing
4.1 Parsing non-verbal predicates

4.2 Machine translation testing
   Here are the translations for lines 15-17:
   
   Cong   i        ani muiny.
   hunger 3sg.real eat dog
   Hunger eats the dogs. (The dogs are hungry.)
   
   Manuai e    poany.
   osprey prep earth
   The osprey is on the earth (The cat is in the park.)
   
   Muiny ala manuai.
   dog   3pl.real osprey
   The dogs are the osprey. (The dogs are the cats.)
   
4.3 Non-verbal predicates in wh questions

We got wh-questions to work, and we also got them to work as nominal predicates.
To do this, we maded it so that no quantifiers were being added in the np-pred
rule. 
===============================================================================
5. Small miscellaneous changes

We continued cleaing up the spurious pronouns by deleting noun41 to
   noun82 EXCEPT noun48 and noun82. Here's an example of one of the
   pronouns we deleted:

   noun41_name=noun41
   noun41_pron=on
     noun41_feat1_name=number
     noun41_feat1_value=du
   noun41_det=opt
     noun41_stem1_orth=damolou
     noun41_stem1_pred=_pron_n_rel

We deleted the alternate spelling on 'pa' as 'poa' and instead added a new
   lexical entry for 'poa' that had the appropriate 2sg agreement (w/ subject)
   features.

In our testsuite.txt, we changed mood glosses from nfut/fut to real/irr.
   This is to reflect our analysis that Titan does not have tense, only mood.

We finished spellling out properites in semi.vpm such as cond -> conditional.

We changed the PRED values for some lexical entries used in machine
   translation. 'manuai' changed from _osprey-or-osprey-feather_n_rel to
   _cat_n_rel, voliliti changed from follow to chase, dilen from canoe to
   car, and lono from forest to park.

We deleted this unwanted auto-inferred wh word:
   ca_1 := noun1-noun-lex &
     [ STEM < "ca" >,
       SYNSEM.LKEYS.KEYREL.PRED "_what_n_rel" ].

We changed the pred value of all pronounes from _pron_n_rel to pron_rel.

===============================================================================
6 Grammar comparision
6.1 Testsuite
   6.1.1 Initial grammar (copied from lab 6)
      56/88 positive items parsed (63.6% coverage)
      6/77 negative items parsed (7.8 overgeneration)
      
      On average:
         positive items that parsed had 1.11 parses.
         negative itmes that parsed had 1.00 parses
      
      The most ambiguous item had 3 parses:
         Ala      kip.
         3pl.real lie
         They lied.
      See lab 6 for an explination of the ambiguity.

   6.1.2 Final grammar
      #/# items parsed.
      #/# positive items parsed (#% coverage)
      #/# negative items parsed (#% overgeneration)
      
      On average:
         Positive items that parsed had # parses.
         Negative items that parsed had # parses.
      
      The most ambiguous items had # parses:
         {IGT}
      {explanation of the ambiguity}

6.2 Test corpus
   6.2.1 Initial grammar (copied from lab 6)
      29/562 items parsed (5.2% coverage)
      
      On average, parsed items had 1.45 prases
      
      The most ambiguous item had 3 parses:
         ala      va  yota     ka  mulie
         3pl.real say 1pl.incl irr run
         They said lets turn around
      See lab 6 for an explanation of the ambiguity.

   6.2.2 Final grammar
      #/# items parsed (#% coverage)
      
      On average, parsed items had # parses.
      
      The most ambiguous item had # parses:
         {IGT}
      {explanation of the ambiguity}
