Lab 7
by David Krug and Nathanial Imel

There are four tsdb profiles in tsdb/home:
   lab_7_initial_corpus - Test corpus with initial grammar
   lab_7_initial_ts     - Testssuite with initial grammar
   lab_7_final_corpus   - Test corpus with final grammar
   lab_7_final_ts       - Testsuite with final grammar

Table of contents:
   1 Nominal predicates
      1.1 How they work in Titan
      1.2 How we implemented them
   2 Adjectival predicates
      2.1 How they work in Titan
      2.2 How we implemented them
   3 Prepositional predicates
      3.1 How they work in Titan
      3.2 How we implemented them
   4 Non-verbal predicate testing
      4.1 Translation testing
      4.2 Wh questions and non-verbal predicates
   5 Small miscellaneous changes
   6 Grammar comparision
      6.1 Testsuite
         6.1.1 Initial grammar
         6.1.2 Final grammar
      6.2 Test corpus
         6.2.1 Initial grammar
         6.2.2 Final grammar


  A predicate can be a noun, as in the equative sentences "he's a snake",
    a prepositional phrase ("part is to me/of mine"), an adjective, or
    a numeral, as well as a verb.
  
  ~ Claire Bowern

===============================================================================
1 Nominal predicates
1.1 How they work in Titan
   There is no copula in Titan, and agreement clitics do not occur with nominal
      predicates. Here is a nominal predicate:
      
      manuai cinal
      osprey devil
      The osprey is a devil.
      
      oi  cinal
      2sg devil
      You are a devil

      *oi a        cinal
      2sg 2sg.real devil
      You are a devil.

1.2 How we implemented them
   At first we copied the n-bar-predicate-rule from Halkomelem in 2013. This
      resulted in the rule over-firing, and after interactive debugging on
      Thursday we added the MOD <> requirement on the HEAD as well as the
      SPEC <> and no new quantifier requirement. This is the final tdl:

   np-predicate-rule := unary-phrase & nocoord &
     [ SYNSEM [ LOCAL.CAT [ HEAD verb & [MOD < >],
                      VAL [ COMPS < >,
         	 SPEC < >,
         	 SUBJ < [ LOCAL [ CONT.HOOK.INDEX #arg1,
         			  CAT [ HEAD noun,
         				VAL.SPR < > ] ] ] > ] ],
                NON-LOCAL #nl ],
       C-CONT [ HOOK [ LTOP #ltop,
             INDEX #index,
             XARG #arg1 ],
           RELS.LIST < arg12-ev-relation &
         	 [ PRED "_be_v_id_rel",
         	   LBL #ltop,
         	   ARG0 #index,
         	   ARG1 #arg1,
         	   ARG2 #arg2 ] >,
           HCONS.LIST < > ],
       ARGS < [ SYNSEM [ LOCAL [ CAT [ HEAD noun,
         		    VAL.SPR < > ],
         	      CONT.HOOK [ INDEX #arg2 ]],
               NON-LOCAL #nl ] ] > ].

   We added the corresponding rule instatiation in rules.tdl:
      np-pred := np-predicate-rule.
   
   An interesting effect of this is that a stand-alone NP will now parse as a
      complete sentence. Because the np-predicate-rule turns NPs into VPs, and
      because subjects are optional, a sentence like "manuai" will parse:
      
      Manuai
      osprey
      Something is being an osprey.

   In that sentences:
   arg1 of be_v_id_rel is nonfocus.
   arg2 of be_v_id_rel is a cat that exists.
   The event on the index of the sentence is a prop-or-question.

===============================================================================
2 Adjectival predicates
For this lab, we did not fully implement predicative adjectives. The auto-
   inference system placed all adjectives in the transitive verb class, but
   they should be in the intransitive verb class. Additionally, we have still
   not decided if we should allow non-third verbal clitics. We currently have
   one toy example of a verb, 'dili' (sad).

2.1 How they work in Titan
   Adjectives can appear attributively or predicatively:
      Yo  u        ani kan  muan
      1sg 1sg.real eat food bad
      I eat bad food
      
      Kan  muan.
      food be.bad
      The food is bad.

   Predicative adjectives mostly act like intransitive verbs and appear in
      verbal slots:
      
      Lodianum ei i likom.
      house-inside 2sg.ps 2sg be.pitch-dark
      The inside of her house was pitch black.
   
   According to the descriptive grammar, the difference between regular
      intransitive verbs and adjectives is that agreement clitics do not appear
      with predicative adjectives if the subject is non-third:
      
      *oi a        muan
      2sg 2sg.real be.bad
      You are bad
      
      Oi  muan
      3sg be.bad
      You are bad
   
   However, the corpus includes the following:
      Yo  u        qadi.
      1sg 1sg.real sick
      I am sick.
    
   Additionally, the descriptive grammar says that predicative adjectives do
      not appear in irrealis or perfective clauses. However, the corpus
      includes:
      
      kor kine alau
      land 3sg.perf far
      "The land is far away.

2.2 How we implemented them
   For this lab, we are analyzing adjectives as a simple intransitive.
      Therefore, they will still accept the agreement clitic in
      non-third person. We might come back to this in the future and ban
      agreement clitics with non-third subjects by using position classes
      like your reccommended in office hours on Friday.
   
   We noticed that all adjective-like verbs, such as 'dili' (sad)
      inherint from verb1-verb-lex, which are transitive verbs. We also noticed
      that all these verbs have a stem that ends in "~", such as:
           
      dili_7E := verb1-verb-lex &
        [ STEM < "dili~" >,
          SYNSEM.LKEYS.KEYREL.PRED "_be_sad_v_rel" ].

   All adjectives should have the "~" removed from their stem and be moved into
      verb3-verb-lex, which is intransitive. We have not yet fixed all these
      adjectives, but we did fix 'dili' (small):

      dili := verb3-verb-lex &
        [ STEM < "dili" >,
          SYNSEM.LKEYS.KEYREL.PRED "_be_sad_v_rel" ].

===============================================================================
3 Prepositional predicates
3.1 How they work in Titan
   The most common adposition in Titan by far is the preposition 'e' which is
      glossed variably as of, in, on, etc:

      nat   e  wei  poen
      child in that not
      The child is not in that [basket].
      
      muiny e  manuai.
      dog   in osprey
      The dog is in the osprey.
   
   Agreement clitics do not appear in the corpus with prepositional predicates:
      *Manuai i   e    muiny.
      osprey  3sg prep dog
      The osprey is a dog
    
   We noticed this PPs showing up at the front of sentences, but we think that
      is not predicative prepositions but instead a different contstruction for
      topicalization:
      
      e         si  pein        i   moele   i
      regarding one young-women 3sg adorned 3sg
      As for one young worman, she adorned herself.

   We are curious how this will interact with NP predicates, such as:
      e muiny manuai
      prep dog osprey.
      Regarding the dog, it is an osprey.
   
   Current all three parses of this sentence mean "On the dog, something is being an
      osprey".

3.2 How we implemented them
   Initially, Titan had no prepositions. We added one preposition:

   e := normadp1-norm-adposition-lex &
     [ STEM < "e" >,
       SYNSEM.LKEYS.KEYREL.PRED "_loc_p_rel" ].

   With this change, PPs such as the following were being partially parsed as
      NPs:      

      muiny e manuai
      dog in osprey
      the dog in the osprey
   
   To get predicative prepositions working, we edited the norm-adposition-lex
      rule to select for a subject and ID its INDEX with the INDEX of the
      adoposition's ARG1. Here is the final tdl for norm-adposition-lex:

   norm-adposition-lex := norm-sem-lex-item & no-hcons-lex-item & basic-intersective-mod-lex & non-local-none-lex-item &
     [ ARG-ST < #comp >,
       SYNSEM [ LKEYS.KEYREL arg12-ev-relation &
                             [ ARG1 #arg1,
         	    ARG2 #arg2 ],
                L-QUE #lque,
                LOCAL [ CONT [ HOOK [ XARG #arg1 ] ],
                        CAT [ WH.BOOL -,
                              VAL [ SPR < >,
                                    SPEC < >,
                                    SUBJ < [ LOCAL [ CAT cat-sat &
                                          	    	    [ VAL [ SPR < >,
                                          	    COMPS < > ],
                                   		     HEAD noun ],
         			          CONT.HOOK.INDEX #arg1 ] ] >,
                                    COMPS < #comp &
                                            [ L-QUE #lque,
         			   OPT -,
                                              LOCAL [ CAT [ HEAD noun,
                                                            VAL.SPR < > ],
                                                      CONT.HOOK.INDEX #arg2 ] ] > ],
                              HEAD adp &
                                   [ MOD < [ LOCAL.CAT [ VAL.SPR cons,
                                                         WH.BOOL -,
                                                         HEAD.AUX - ] ] > ] ] ] ] ].
   
   normadp1-norm-adposition-lex := norm-adposition-lex &
     [ SYNSEM.LOCAL.CAT.HEAD.INIT + ].

   We also changed roots.tdl to have HEAD +vp instead of just HEAD verb.
      
   With these changes, "muiny e manuai" had 7 parses. One was the desired
      parse:
      
      Muiny e  manuai.
      dog   in osprey.
      The dog is in the osprey.
   
   Of the other 6 parses, some had the over-firing np-predicate problem that
      we fixed in section 1.2 rule, and some parses had the HEAD-ADJ or the
      ADJ-HEAD rule applying. To fix this, we added OPT - to the first item
      on the COMPS list of the norm-adpos-lex.

   This reduced the number of parses of "muiny e manuai" to 3 desired parses
      with 3 structures:
      
      Something is being "the dog on the cat".
      Something is being the dog, and that being is happening on the cat.
      A dog is on the cat. (The top node in this tree is a PP)

===============================================================================
4 Non-verbal predicate testing
4.1 Machine translation testing
   Here are the translations for lines 15-17:
   
   Cong   i        ani muiny.
   hunger 3sg.real eat dog
   Hunger eats the dogs. (The dogs are hungry.)
   
   Manuai e    lono.
   osprey prep forest
   The osprey is in the forest (The cat is in the park.)
   
   Muiny ala manuai.
   dog   3pl.real osprey
   The dogs are the osprey. (The dogs are the cats.)

   When translating "manuai e lono", three pareses were found and 47
      translations generated. Here are the three pares:

      E lono manuai
      Manuai e lono
      E lono manuai

   Here is an example of one of the 47 translations:
      Aru e lono manuai
      1dl prop forest cat.
      We are the cat in the forest

   We currently are not sure why pronouns like 1dl are showing up as
      translations for a sentence that has no 1dl prononus.
  
4.2 Non-verbal predicates in wh questions
   Initially wh-questions were not working at all. We got them to work by
      answering the appropriate questions on the cusomization site. To get
      wh-questions working as nominal predicates, we made it so that no
      quantifiers were being added in the np-pred rule. Here is the final tdl:
    
   wh-pronoun-noun-lex := basic-wh-word-lex & norm-hook-lex-item & basic-icons-lex-item & non-mod-lex-item & zero-arg-que &
     [ SYNSEM [ LOCAL [ CAT [ HEAD noun,
                              VAL [ SPR < >,
                                    SUBJ < >,
                                    COMPS < >,
                                    SPEC < > ] ],
                        CONT [ RELS.LIST < [ LBL #larg,
                                             ARG0 #arg0 & ref-ind ],
                                           quant-relation &
                                           [ PRED "which_q_rel",
                                             ARG0 #arg0,
                                             RSTR #harg ] >,
                               HCONS.LIST < [ HARG #harg,
                                              LARG #larg ] > ] ],
                NON-LOCAL.QUE.LIST < #arg0 > ] ].
===============================================================================
5. Small miscellaneous changes

We continued cleaing up the spurious pronouns by deleting noun41 to
   noun82 EXCEPT noun48 and noun82. Here's an example of one of the
   pronouns we deleted:

   noun41_name=noun41
   noun41_pron=on
     noun41_feat1_name=number
     noun41_feat1_value=du
   noun41_det=opt
     noun41_stem1_orth=damolou
     noun41_stem1_pred=_pron_n_rel

We deleted the alternate spelling on 'pa' as 'poa' and instead added a new
   lexical entry for 'poa' that had the appropriate 2sg agreement (w/ subject)
   features.

In our testsuite.txt, we changed mood glosses from nfut/fut to real/irr.
   This is to reflect our analysis that Titan does not have tense, only mood.

We finished spellling out properites in semi.vpm such as cond -> conditional.

We changed the PRED values for some lexical entries used in machine
   translation. 'manuai' changed from _osprey-or-osprey-feather_n_rel to
   _cat_n_rel, voliliti changed from follow to chase, dilen from canoe to
   car, and lono from forest to park.

We deleted this unwanted auto-inferred wh word:
   ca_1 := noun1-noun-lex &
     [ STEM < "ca" >,
       SYNSEM.LKEYS.KEYREL.PRED "_what_n_rel" ].

We changed the pred value of all pronounes from _pron_n_rel to pron_rel.

===============================================================================
6 Grammar comparision
6.1 Testsuite
   6.1.1 Initial grammar (copied from lab 6 final)
      57/100 positive items parsed (57% coverage)
      6/85 negative items parsed (7.1% overgeneration)
      
      On average:
         positive items that parsed had 1.11 parses.
         negative itmes that parsed had 1.00 parses
      
      ala kip
      3pl lie
      "They lie."

      Pron/clitic ambiguity. See lab 6 for more details.

   6.1.2 Final grammar
      # items parsed.
      72/100 positive items parsed (72% coverage)
      11/85 negative items parsed (12.9% overgeneration)
      
      On average:
         Positive items that parsed had 1.42 parses.
         Negative items that parsed had 1.27 parses.
      
      The most ambiguous items had 4 parses:

           *Manuai i   e    muiny
     osprey 3sg prep dog
     "The eagle is on the dog."

  In all four of these parses, "i" is being picked up
     as a noun. Then the np-predicate rule and the PPs are
     combining in various ways. We think it's possible that
     some of these analyses are correct, but the example
     was intended as a negative case.

  The first tree, for example, has an NP being built out of
     an NP and a PP via the SUBJ-HEAD rule. Then the head PP
     modifies "manuai" to produce an N by the HEAD-ADJ rule.
     We think this might be bogus, since it seems to have the
     meaning "something is being an eagle modified(?) by he(?)
     is on the dog."

  We're not sure how to block parses like
     these, since they would seem to be also licensing our
     earlier parse for "manuai e muiny" where "manuai" was an S.
     (from the end of section 3 of this writeup).

6.2 Test corpus
   6.2.1 Initial grammar (copied from lab 6 final)
      29/562 items parsed (5.2% coverage)
      
      On average, parsed items had 1.45 prases
      
      The most ambiguous item had 3 parses:
         ala      va  yota     ka  mulie
         3pl.real say 1pl.incl irr run
         They said lets turn around
      See lab 6 for an explanation of the ambiguity.

   6.2.2 Final grammar
      44/562 items parsed (7.8% coverage)
      
      On average, parsed items had 1.95 parses. (increase)
      
      The most ambiguous item had 9 parses:

     ala nyak  e  kor  e    ala cinal
     3pl climb to home poss 3pl devil
     They climbed into the devils lair.
    
   This IGT came from the corpus. We haven't handled "e" as a possessive
      strategy yet. Regarding its ambiguity, we note that "ala" is parsed
      as a verb in two trees. The rest of the ambiguity seems to be coming
      from the many alternate constructions for VPs and the PPs.

   We note that even simple sentences using "e" as an
      adposition have 3 trees ("muiny e manuai"), due to ambiguity between
      having PP as the root node and the two parses where the np-predicate
      rule is firing.

   One nice thing is that we seem to be parsing longer sentences now
      that we've added "e" as an adposition.
