Lab 5 Writeup
By David Krug and Nathaniel Imel

possible phenomena:
	wh questions
	adverbial modifiers
	clausal complements
	irrealis clitics
	conjunctions

questions for emily
	-transitive-clausal-comps1/2, case unspecefied
	-question verbs
	-are sub morphemes a head or adverb
===============================================================================
1. We improved the choices file for five phenomena:
	coordination, irrealis mood, clausal complements, adverbial clausal modifiers, wh question words
   The last 3 of these we wrote testsuite examples for last week.
		
	1.1 Cordination
		Phenomenon: titan has only polysyntedic coordination
		
		Moat  i        tawi   kor, pe  i          tawi   kan  pe  i        tawi yota
		snake 3sg.nfut create land and 3sg.create create food and 3sg.nfut create1pl.inc
		The snake created the land, food, us.
		
		Analysis: we removed all coordination except polysyntedton
		what we changed in the choices file: copy-paste
	
	1.2 Irrealis mood
		prose description of the phenomenon: ku, ka, ku, __
		prose description of our analysis: we addeded four auxiliary verb types and gave them restrictions on mood
		IGT
		what we changed in the choices file: copy-past
	
	1.3 Clausal complements
		prose description of the phenomenon: va, pa~poa
		prose description of our analysis: we edit the page "clausal complements"
		IGT
		changes made in the choices file (paste in the actual choices)

	1.4 Adverbial clausal modifiers
		prose description of the phenomenon: alan/if
		prose desctiption of our analysis: ?
		IGT
		changes made: copy-paste

	1.5 Wh question words
		prose description of the phenomenon: ca/that, etc.
		prose description of our analysis: we added nouns and made them questions
		IGT
		changes made: copy-paste

===============================================================================
2. Translating the MMT sentences
	What are the translations?
	How did we get those translations?
	Are any sentences impossible to translate?

===============================================================================
3. Setting up machine translation
	What happened when we tried the MT set up?
	What difficulties did we encounter?	How did you resolve them?
	What output did we get?

===============================================================================
4. Grammar performance comparison
	4.1. Initial grammar with testsuite (copied from Lab 4)
		43/82 positive items parse (52.4% coverage)
		30/72 negative itmes parse (41.7% overgeneration)
		
		On average, parsed items had 1.28 parses.
			
		Most ambiguous run: Same as initial run, the most ambiguous sentence
			was "Ala kip" which got 3 parses. From the new test suite
			sentences, the most ambiguous sentence was "Yoru ki kip" which
			got two parses. This was ambiguous because "yoru" was getting
			parsed as a pronoun and a verbal clitic.
			
		Sources of ambiguity: Same as initial run, verbal clitics and and
			pronouns get confused.
		
	4.2 Initial grammar with corpus (same as Lab 3)
	    		1.2.1 How many items parsed?
			      24 out of 563 positive items (4.3%)
			      
		4.2.2 What is the average number of parses per parsed item? 1.50
		4.2.3 How many parses did the most ambiguous item receive?
		      There were two sentences that received 3 trees:
		      
		      'Ala iri i.' and 'Ara vavuen.'
		      
		4.2.4 What sources of ambiguity can you identify?
		
		      Mostly the pronoun/clitic confusion.
	    
	4.3 Final grammar with testsuite
	4.4 Final grammar with corpus (n/a)
	
