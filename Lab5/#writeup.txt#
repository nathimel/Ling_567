Lab 5 Writeup
By David Krug and Nathaniel Imel

===============================================================================
1. We improved the choices file for five phenomena:
   - coordination
   - irrealis mood
   - clausal complements
   - adverbial clausal modifiers
   - wh question words
   The last 3 of these we wrote testsuite examples for last week.
      
   1.1 Cordination
      Phenomenon: Titan has only polysyntedic coordination. There are two
           coordinators: 'pe' ('or') and 'ne' ('or'). 'Ne' can coordinate NPs
           or VPs, 'pe' can coordinate only NPs. The coordinators appear before
           the coordinands.
      
      # Polysyndeton coordination
      Moat  i        tawi   kor pe  i        tawi   kan  pe  i        tawi   yota
      snake 3sg.nfut create land and 3sg.nfut create food and 3sg.nfut create 1pl.inc
      The snake created the land, food, and us.
      
      # Titan does not have monosyndeton coordination
      *Moat  i        tawi   kor  i        tawi   kan  pe  i        tawi   yota
      snake 3sg.nfut create land 3sg.nfut create food and 3sg.nfut create 1pl.inc
      The snake created the land, food, and us.
      
      Our analysis: We removed all coordination strategies previously in the
           choices file and added two. The first covers 'pe' and allows NP or
           VP coordination. The second strategy covers 'ne' and allows only VP
           coordination.
      
      Choices file:
      section=coordination
        cs1_np=on
        cs1_vp=on
        cs1_pat=poly
        cs1_mark=word
        cs1_orth=pe
        cs1_order=before
        cs2_np=on
        cs2_pat=poly
        cs2_mark=word
        cs2_orth=ne
        cs2_order=before
   
   1.2 Irrealis mood
      Phenomenon: There are four verbal agreement clitics that mark irrealis
         mood as well as agree for person and number: ku, ko, ki, ka
      
      # Agreement between 'Taru' and 'ka'
      Source: author
      Vetted: f
      Judgement: g
      Phenomena: {tense, modals}  
      Taru     ka       kip.
      1dl.incl n.sg.fut lie
      We will lie.
      
      # Failed agreement between 'I' and 'ku'
      Source: author
      Vetted: f
      Judgment: u
      Phenomena: {tense, modals}
      I   ku      kip
      3sg 1sg.fut lie
        
      Our analysis: We addeded four auxiliary verb types and gave them
         restrictions on mood. We haven't gotten person and number working yet.
      
      Choices file: <copy-paste lines 2571-2603 from choices file>
   
   1.3 Clausal complements
      Phenomenon: There are some verbs in Titan that take clauses as a
         complement. Most of these do not make any specification about the
         mood of of the complement clause, except 'pa~poa' ('want'), which
         requires that its compelement clause be irrealis.
      If a verb takes a clausal complement in either mood, the mood can
         affect the meaning of the verb.
        
      # 'va' taking a clausal comp with realis mood
      Ato va  Oi  a        kip
      3pc say 2sg 2sg.nfut kip
      They said, "You're lying!".
      
      # 'va' taking a clausal comp with irrealis mood
      I va ki puti yo
      3sg want 2sg.fut marry 2sg
      He wants to marry me.
      
      # 'pa~poa' requires irrealis
      Yo  pa   ku      le    Yap
      1sg want 1sg.fut go.to Yap
      I want to go to Yap.
     
      # 'va' can not take only an NP
      Ato va yoito
      3pc said 1pc.excl.
      They said, "We".
      
      Our analysis: We edited the page Clausal Complements and added two
         clausal complement stratgies. The first covers most verbs that
         take a clausal complement, and it does not restrict the value of mood.
         The second strategy covers verbs like 'pa~poa' and constrains the mood
         value of the clausal comlement.
      
      Choices file:
      section=clausal-comp
        cmops1_clause-pos-same=on
        cmops1_ques=prop
        cmops2_clause-pos-same=on
        cmops2_ques=prop
          comps2_feat1_name=mood
          comps2_feat2_value=irr

   1.4 Adverbial clausal modifiers
      Phenomenon: Titan has adverbial modifies, like 'alan' ('if/before'),
         which take the form of a free subordination morpheme that occurs
         before the matrix clause and attaches to/modifies sentences. The
         meaning of 'alan' depends on the mood of the <subordinate|matrix>
         clause.
        
      # 'alan' irrealis marks conditional
      Source: author
      Vetted: f
      Judgement: g
      Oi  ko      kip alan yo  ku      kip
      2sg 2sg.fut lie if   1sg 1sg.fut lie
      If you lie, I will lie.
      
      # 'alan' with realis marks causitive
      Source: author
      Vetted: f
      Judgement: g
      Yo ulumui das alan, das i camar.
      1sg drink sea because sea 3sg dry.up
      Because I drank the sea, the sea water has dried up.
      
      # 'alan' can not appear after the matrix clause
      Oi ko kip yo ku kip alan.
      2sg 2sg.fut lie 2sg 2sg.fut lie if.
      You will lie if I will lie.
      
      Our analysis: We added two Clausal Modifier strategies to cover the two
         different uses of 'alan'. The first strategy __
      
      Choices file:
      section=clausalmods
        cms1_position=before
        cms1_modifier-attach=2
        cms1_subordinator=free
        cms1_subposition=after
        cms1_subordinator-type=head
          cms1_=freemorph1_orth=alan
          cms1_=freemorph1_pred=_if_subord_rel
          cms1_feat1_name=mood
          cms1_feat1_value=irr
        cms2_position=before
        cms2_modifier-attach=2
        cms2_subordinator=free
        cms2_subposition=after
        cms2_subordinator-type=head
          cms2_=freemorph1_orth=alan
          cms2_=freemorph1_pred=_if_subord_rel
          cms2_feat1_name=mood
          cms2_feat1_value=realis

   1.5 Wh question words
      Phenomenon: Titan has several interrogative pronouns that occur in situ.
        
      # 'ca' in situ.
      Source: author
      Vetted: f
      Judgment:g
      Phemomena: {wh-question}
      Oi a ani ca
      
      # 'ca' not in situ
      Source: author
      Vetted: f
      Judgment: u
      Phenomena: {wh-questions}
      Ca   oi  a        ani
      what 2sg 2sg.nfut eat
      What are you eating?
        
      Our analysis: We added several nouns types and made them each questions
         pronouns. Each question pronoun has an optional determiner, and no
         morphotactic constraints.
      
      Choices file:

  aux39_name=aux39
  aux39_sem=no-pred
    aux39_feat1_name=mood
    aux39_feat1_value=irr
    aux39_feat1_head=verb
  aux39_subj=np
    aux39_compfeature1_name=form
    aux39_compfeature1_value=finite
    aux39_stem1_orth=ku
  aux40_name=aux40
  aux40_sem=no-pred
    aux40_feat1_name=mood
    aux40_feat1_value=irr
    aux40_feat1_head=verb
  aux40_subj=np
    aux40_compfeature1_name=form
    aux40_compfeature1_value=finite
    aux40_stem1_orth=ku
  aux41_name=aux41
  aux41_sem=no-pred
  aux41_subj=np
    aux41_compfeature1_name=form
    aux41_compfeature1_value=finite
    aux41_stem1_orth=ki
  aux42_name=aux42
  aux42_sem=no-pred
    aux42_feat1_name=mood
    aux42_feat1_value=irr
    aux42_feat1_head=verb
  aux42_subj=np
    aux42_compfeature1_name=form
    aux42_compfeature1_value=finite
    aux42_stem1_orth=ka
   
   1.6 Miscellaneous choices changes
      There were a few minor cleanup changes that we made to the choices file.
      
      1.6.1 Removed Case
         The initial choices file from the start of Lab 2 included case, which
            which Titan does not have. We removed case, which required us to
            also edit many of the aux verb types to remove their constraint on
            case..

===============================================================================
2. Translating the MMT sentences
   Of the 26 sentences in eng.txt, we translated 18 of them. Of the 8 we
      skipped, 3 of them we determined to be impossiblee to translate.
   
   2.1 Example translations
      Here are the first three sentences we translated:
      
      Muiny ala      metir.
      dog   3pl.nfut sleep
      Dogs sleep.
      
      Muiny ala      voliliti dilen.
      dog   3pl.nfut follow   canoe
      Dogs follow canoes.
      
      Yo  u        voliliti oi.
      1sg 1sg.nfut follow   you.
      I follow you.
   
   2.2 Mis-translatated words
      Three of the English words in eng.txt do not have Titan translations, so
         we replaced those words with semantically similar words as follows:
      chase - follow (voliliti)
      cat   - osprey (manuai)
      car   - canoe (dilen)
   
   2.3 Impossible translations
      The three sentences we found to be impossible to translate were the three
         sentences with the word 'ask'. In Titan, the word for ask is 'va',
         which requires that there be a vocative or adressee of the question.
   
      Therefore, "I asked what the dogs chased" is not translatable, but
         "I asked 'John, what did the dogs chase?'" is translatable.
   
   2.4 Other skipped translations
      Of the five sentences that we skipped but that aren't impossible to
         translate, three of them have words which we have no translation
         for: 'think', 'know', and 'after'.
      The other two that we skipped have phenomena that we haven't yet
         implemented: prepositions and yes/no questions.

===============================================================================
3. Setting up machine translation
   What happened when we tried the MT set up?
   3.1 What difficulties did we encounter? How did you resolve them?
       There were syntax errors in our trigger.mtr file, in two places. The
       following steps resolved the errors and allowed the grammar to compile
       with ace.

       In ki_3_gr, I deleted the part of the entry that had:
          MOOD oblig, cond
       In a_2_gr, I deleted the part of the entry that had:
          TENSE ipfv, pfv
          
   3.2 What output did we get?
       We received many duplicate strings. For eng to ttv, we received 64 strings
       for the first line. For the second sentence, 0 sentences were generated
       because _car_n_rel and _chase_v_rel were unknown in the semantic index.
       The third sentence was also not generated because _chase_v_rel and pron_rel
       were unknown. But we should be able to fix pron_rel soon.

       Furthermore, there was a difference between ttv to ttv and eng to ttv. In
       fact, ttv to ttv yielded 192 strings while eng to ttv yielded 64. And we
       know that there are two parse trees for the Titan sentence, though they
       have the same semantics. We'd be very interested to know whether the
       eng to ttv translation is using just one of these trees, and the reason
       for this big difference in general.

       Also, all of the eng to ttv sentences had only the irrealis
       clitic. Not a single sentence had a realis clitic, which we thought was
       strange.

===============================================================================
4. Grammar performance comparison
   4.1. Initial grammar with testsuite (copied from Lab 4)
      43/82 positive items parse (52.4% coverage)
      30/72 negative itmes parse (41.7% overgeneration)
      
      On average, parsed items had 1.28 parses.
         
      Most ambiguous item: Same as initial run, the most ambiguous sentence
         was "Ala kip" which got 3 parses. From the new test suite
         sentences, the most ambiguous sentence was "Yoru ki kip" which
         got two parses. This was ambiguous because "yoru" was getting
         parsed as a pronoun and a verbal clitic.
         
      Sources of ambiguity: Same as initial run, verbal clitics and and
         pronouns get confused.
      
   4.2 Initial grammar with corpus (copied from Lab 4)
      24/563 positive items (4.3% coverage)
               
      On average, parsed items had 1.50 parses.
      
      Most ambiguous item:
         There were two sentences that received 3 trees:
         'Ala iri i.' and 'Ara vavuen.'
            
      Sources of ambiguity:
         Mostly the pronoun/clitic confusion.
       
   4.3 Final grammar with testsuite
     106/157 items parsed.
     65/84 positive items parsed (77.4% coverage)
     41/73 negative items parsed (56.2% overgeneration)
     
       2. What is the average number of parses per parsed item?
            1.85 for positive, 1.76 for negative
            
       3. How many parses did the most ambiguous item receive?

            This sentence received 8 distinct analyses;

         i va ki puti yo
         3sg want 3sg.fut marry 1sg
         'He wants to marry me'

         4 are due to analyzing 'i' as a verb instead of a pronoun. Olga has just made it possible for us to fix
         this, but we'll leave it to next lab. Of the 4 that analyzed 'i' as a verb, 2 analyzed 'ki' as a noun
         with a ridiculously long predication of all of the IGT glosses that were actually the many irrealis
         meanings for 'ki.' So we will delete that lexical entry, and only our own 'ki' verb agreement clitic
         for irrealis should remain. 
         
       4. What sources of ambiguity can you identify?

            In addition to the pronoun/clitic ambiguity and multiple lexical entries for the same item, we have
     structural ambiguity in some of the parses. For example, in the VP structure for a phrase like
     'ku mat', there are analyses that label 'ku' or 'mat' as higher, or sometimes each as Vs (and not VPs).

       5. For 10 items (if you have at least that many parsing), do any of the parses look reasonable in the semantics?

            This will be a diff to the previous labs' semantics:

     1.  i va ki puti yo
         3sg want 3sg.fut marry 1sg
         'He wants to marry me'


     This was explored above as the most ambiguous item. There were two parses that were mostly reasonable
     however -- we just need to get rid of a lexical entry and implement agreement on the pronouns. However,
     there is an issue with the linking of ARG0 of the verb 'die' to the ARG2 of the 'say' verb. The same
     issue comes up with the verb 'pa' and we talk about it in more detail in 4.

     2.
        Manuai i va  yo  ku      mat
        Eagle  say 1sg 1sg.fut die
        'Eagle said, "I will die."

    This sentence has 6 parse trees, but as far as we can tell they all have the same semantics. Also, the
    semantics look very reasonable -- we think we implemented the clausal complement-embedding word 'va'
    correctly here. It just needs to have the clitic 'i' have its predication be empty, rather than
    _3sg.nfut_n_rel.

    3. ato va
       3pc say
       *They said

    This string received 3 parse trees, though it was meant to be a negative example. We think the reason is that
    we need to delete the entry for 'va' that's being used, which is the automatically generated from the IGT
    version that is transitive and its predication is _3sg_thing-or-say-or-want_v_rel.

    4.
       Yo u pa ku kip.
       1sg 1sg.nfut want 1.sg.fut lie
       I want to lie.

    This sentence has 5 trees, and we only want one. Two of the trees parse 'ku' as a noun with a long predication,
    which we think was automatically generated from the IGT. In the trees parsing 'ku' as the aux verb, the ARG0
    of the 'lie_v_rel' item is an event e11 identified with the the LARG value of ICONS. And the ARG2 of the
    'want_v_rel', which we think should be identified with the ARG0 of the lie_v_rel', is instead identified with
    the HARG feature's value of the second qeq on the HCONS list.

    We think this is not quite right, and may have
    to do with incorrectly implementing clausal complement semantics. Perhaps there's something in the customization
    system we can do to fix this? We actuallly received 5 parses for every 'pa' test case, and they all seem to be
    suffering from this issue.

    5. Muiny ala matir
       Dog   3pl sleep
      'Dogs sleep.'

      This is our first MT sentence, and the semantics all look good. All ARGns look appropriate, including ARG0
      of the _sleep_rel being linked to LARG of a qeq member on HCONS that has the HARG value that's identified
      with the ARG1 of the auxiliary 'ala'. Of course, the actual predication on 'ala' is incorrect (_3pl_v_rel)
      but we'll fix that soon. There are two parse trees for this sentence, but they have the same semantics
      because they only vary in the VP structure.

    6. muiny ala voliliti dilen
       dog   3pl follow   canoe
       'Dogs follow the canoe'

    Our second MT sentence has just one parse tree, and the semantics look reasonable with the correct
    linking of ARGns, including qeq linking.

    7. Yo u voliliti oi
       1sg 1sg follow 2sg
       'I follow you.'

   Third MT sentence has one parse tree, and the semantics looks reasonable like the preivous two.

   8. Yo pe   oi  yoru     kip
      1sg and 2sg 1dl.incl lie
      'I and you lie'

   This is an example of coordination that is working well. All our coordination examples that did parse
   received just one tree, and the semantics look reasonable, with ARGns being linked correctly. Unfortunately,
   only our pe (and) cases parsed, and none of our ne (or) cases did.

   9. yaye-yo     i  ani lei
      mother-1sg 3sg eat ginger-bulb
      'My mother eats a ginger bulb.'

   The ARG1 of the poss_rel is the mother here, and the ARG2 is the pronoun. If ARG1 is supposed to be the
   possessum and ARG2 is to be the possessor, that sounds right. And the mother is being linked to the ARG0
   of the eat_v_rel, which is a good sign.

   10. Oi a ani ca
       2sg 2sg eat what
       'What are you eating?'

   This was the only wh-question test case that parsed (besides 'oi se', which we aren't reporting on because
   it is a test case exhibiting too much phenomena at once). The semantics shows that the predication for 'ca'
   is _what_n_rel, suggesting that the sentence parsed using the automatically generated lexical entry for 'ca'
   from the IGT rather than our own implemented wh-word, which should have the _thing_n_rel predication. Also
   Olga said that for wh-words, the quantifier should be a 'which' instead of an 'exists,' but we have the latter.

   4.4 Final grammar with corpus

      38/563 positive items (6.7% coverage) So we parsed 14 more sentences!
               
      On average, parsed items had 2.89 parses. So we almost doubled our ambiguity.
      
      Most ambiguous item:
         Much more ambiguity. We have sentences that have 4 and 10 and 15 parses.

       The sentence with 15 parses is:

         ala va  yota    ka   mulie
         3pl say 1pl.inc hort turn
         'They said let s turn around'


       10 parses:

         i va awa ka la
         3sg say 2pl hort go
         'He said you go.'

         ato va i kine me
         3pl say 3sg 3sg.perf come
         'They said he s come.'

    These sentences are using the complement embedding verb 'va' and the irrealis verb clitic 'ka.' We know there
    are two entries for 'va'. There only seems to be one entry for 'ka', though. My guess is that there's lots
    of structural ambiguity in how the VP trees are constructed.
       
